\documentclass{res}
\setlength{\textheight}{8.6in} % increase text height to fit resume on 1 page
\newsectionwidth{0pt}  % So the text is not indented under section headings
\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,anchorcolor=blue,citecolor=blue]{hyperref}
\usepackage{enumitem}
\usepackage{zh_CN-Adobefonts_external} % Simplified Chinese Support using external fonts (./fonts/zh_CN-Adobe/)

\newenvironment{my_item}{
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}}
{\end{itemize}
}

\begin{document}

% Center the name over the entire width of resume:
 \moveleft.5\hoffset\centerline{\LARGE\bf 张 雨 辰}
% Draw a horizontal line the whole width of resume:
 \moveleft\hoffset\vbox{\hrule width\resumewidth height 1pt}\smallskip
% address begins here
\textbf{电子邮件}: zhangyuc@gmail.com \hfill \textbf{电话}: (+1)-510-423-1353

\begin{resume}

{\Large\bf 教育背景}

\vspace{-5pt}
\textbf{加州大学伯克利分校 (UC Berkeley)}\hfill\textbf{2011 - 2016}\\
计算机科学博士\\
导师: Michael I. Jordan,  Martin J. Wainwright \\
研究方向: 分布式机器学习算法、优化算法、深度学习算法和理论

\vspace{-5pt}
\textbf{加州大学伯克利分校 (UC Berkeley)} \hfill\textbf{2011 - 2013}\\
统计学硕士

\vspace{-5pt}
\textbf{清华大学}\hfill\textbf{2007 - 2011}\\
计算机科学与技术学士\\
{计算机科学实验班 (姚班)}

{\Large\bf 工作背景}

\vspace{-5pt}
{\bf 主任研究员 (Principal Researcher),  微软}\hfill\textbf{2018 - 现在}
\begin{my_item}
\item 为微软公司搭建下一代的对话系统框架。这套全新的技术让用户可以像和人类伙伴对话一样，自然地、
结合上下文地、协作性地与智能助手进行对话，共同完成复杂的任务。我们的技术服务众多的微软产品
(链接: \href{https://v.youku.com/v_show/id_XNDc0Nzg5MDc2MA}{来自微软CEO的产品演示}; \href{https://www.microsoft.com/en-us/research/project/semantic-machines}{我们的团队})。

\item 我领导了对话系统中的核心对话引擎的设计和实现。该对话引擎支持任意复杂的自然语言理解，并
包含全新的对话状态管理系统，以驱动流畅的多轮次、跨领域对话
(链接: \href{https://zhuanlan.zhihu.com/p/245081650}{对话引擎的设计})。

\item 我还领导了对话产品的国际化: 包括多语言的数据收集、语义理解和语言生成系统等，以服务美国
以外的全部市场。
\end{my_item}
\vspace{-5pt}

{\bf 资深研究员,  Semantic Machines Inc.}\hfill\textbf{2018}
\begin{my_item}
\item 我们是一个由世界级的研究员和工程师们组成的创业团队。我们的使命是开发下一代的对话式人工智能
技术；2018年6月被微软收购。
\end{my_item}

{\bf 博士后研究员,  斯坦福大学}\hfill\textbf{2016 - 2018}
\begin{my_item}
\item 面向维基百科的问答系统: 比当时最好的系统快10倍，并提升10\%的准确率。
\item 深度学习理论: 获2017年COLT会议 (机器学习理论的最高学术会议) 最佳论文奖。  
\end{my_item}

{\bf\Large 研究背景}

我的研究兴趣包括自然语言处理、机器学习算法、分布式算法、搜索和推荐系统等。

\paragraph{自然语言处理}
\begin{my_item}
\item 任务型对话系统 [\ref{task-tacl}, \ref{updating-patent}, \ref{reponse-patent}].
\item 语义解析模型 [\ref{macro-emnlp17}].
\end{my_item}

\vspace{-5pt}
\paragraph{机器学习和优化算法}
\begin{my_item}
\item 深度学习中的非凸优化算法 [\ref{convexified-icml17},\ref{a-hitting-colt17},\ref{on-the-learnability-aistats17},\ref{local-nips16},\ref{l1-icml16}].
\item 凸优化算法 [\ref{stochastic-jmlr},\ref{stochastic-icml15}].
\item 众包算法 [\ref{spectral-jmlr},\ref{spectral-nips14}].
\item 对抗性机器学习 [\ref{defending-aistats17}].
\item 个性化推荐模型 [\ref{taxonomy-wsdm14}].
\item 搜索和在线广告 [\ref{understanding-wsdm12},\ref{user-kdd11},\ref{characterize-www11},\ref{learning-cikm10},\ref{explore-cikm10},\ref{incorporating-sigir10}]
\end{my_item}

\vspace{-5pt}
\paragraph{分布式计算}
\begin{my_item}
\item 分布式系统中的机器学习算法 [\ref{divide-jmlr},\ref{communication-jmlr},\ref{distributed-icml15},\ref{communication-icml15},\ref{divide-colt13},\ref{communication-nips12}].
\item 分布式计算的基础理论 [\ref{lower-colt14},\ref{information-nips13}].
\item 随机算法 (如随机梯度下降) 的并行化编程框架 [\ref{splash}].
\end{my_item}

\vspace{-5pt}
\paragraph{其他研究项目}
\begin{my_item}
\item 理论统计学 [\ref{a-note-ejs},\ref{optimality-ejs},\ref{on-bayes-jmlr}].
\item 理论计算机科学 [\ref{the-antimagicness-tcs},\ref{a-new-isaim10}].
\end{my_item}

{\bf\Large 专利}
\vspace{5pt}

\begin{enumerate}[label={[P\arabic*]}, ref={P\arabic*}]
\item \label{updating-patent} \textbf{Y. Zhang}, J. Wolfe, A. Pauls and D. Hall. Updating Constraints For Computerized Assistant Actions. \emph{U.S.~Patent}, Filed by Microsoft on March 30, 2020.
\item \label{reponse-patent} J. Andreas, A. Vorobev, A. Guo, J. Krishnamurthy, J. Bufe, J. Rusak and \textbf{Y. Zhang}. Response Generation For Conversational Computing Interface, \emph{U.S.~Patent}, Filed by Microsoft on August 12, 2019.
\end{enumerate}

{\bf\Large 期刊论文}
\vspace{5pt}

\begin{enumerate}[label={[J\arabic*]}, ref={J\arabic*}]

\item \textbf{Y. Zhang} and the Semantic Machines team (alpha-beta order). Task-Oriented Dialogue as Dataflow Synthesis.
\emph{Transactions of the Association for Computational Linguistics}.\label{task-tacl}

\item \textbf{Y. Zhang} and L. Xiao. Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization.
\emph{Journal of Machine Learning Research}.\label{stochastic-jmlr}

\item X. Chen, A. Guntuboyina and \textbf{Y. Zhang}. A note on the approximate admissibility of regularized estimators in the Gaussian sequence model.
\emph{Electronic Journal of Statistics}.\label{a-note-ejs}

\item \textbf{Y. Zhang}, M. Wainwright and MI. Jordan. Optimal prediction for sparse linear models? Lower bounds for coordinate-separable M-estimators.\label{optimality-ejs}
\emph{Electronic Journal of Statistics}.

\item X. Chen, A. Guntuboyina and \textbf{Y. Zhang} (alpha-beta order). On Bayes Risk Lower Bounds.
\emph{Journal of Machine Learning Research}.\label{on-bayes-jmlr}

\item \textbf{Y. Zhang}, X. Chen, D. Zhou and MI. Jordan. Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing.
\emph{Journal of Machine Learning Research}.\label{spectral-jmlr}

\item \textbf{Y. Zhang}, J. Duchi and M. Wainwright. Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates.
\emph{Journal of Machine Learning Research}.\label{divide-jmlr}

\item \textbf{Y. Zhang}, J. Duchi and M. Wainwright. Communication-Efficient Algorithms for Statistical Optimization.
\emph{Journal of Machine Learning Research}.\label{communication-jmlr}

\item \textbf{Y. Zhang} and X. Sun. The Antimagicness of the Cartesian Product of Graphs.
\emph{Theoretical Computer Science}.\label{the-antimagicness-tcs}
\end{enumerate}

{\bf\Large 会议论文}
\vspace{5pt}

\begin{enumerate}[label={[C\arabic*]}, ref={C\arabic*}]

\item  \textbf{Y. Zhang}, P. Liang. Defending Against Whitebox Adversarial Attacks via Randomized Discretization.
\emph{Artificial Intelligence and Statistics (AISTATS)}, 2019. \label{defending-aistats17}

\item  \textbf{Y. Zhang}, P. Pasupat, P. Liang. Macro Grammars and Holistic Triggering for Efficient Semantic Parsing.
\emph{Empirical Methods on Natural Language Processing (EMNLP)}, 2017. \label{macro-emnlp17}

\item  \textbf{Y. Zhang}, P. Liang, M. Wainwright. Convexified Convolutional Neural Networks.
\emph{International Conference on Machine Learning (ICML)}, 2017. \label{convexified-icml17}

\item  \textbf{Y. Zhang}, P. Liang, M. Charikar. A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics.
\emph{Conference on Learning Theory (COLT)}, 2017 {\bf (Best paper award)}. \label{a-hitting-colt17}

\item \textbf{Y. Zhang}, JD. Lee, M. Wainwright and MI. Jordan. On the Learnability of Fully-connected Neural Networks.
\emph{Artificial Intelligence and Statistics (AISTATS)}, 2017. \label{on-the-learnability-aistats17}

\item C. Jin, {\bf Y. Zhang}, S. Balakrishnan, M. Wainwright, MI. Jordan.  
Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences.
\emph{Neural Information Processing Systems (NIPS)}, 2016. \label{local-nips16}

\item \textbf{Y. Zhang}, JD. Lee, MI. Jordan. $\ell_1$-regularized Neural Networks are Improperly Learnable in Polynomial Time.
\emph{International Conference on Machine Learning (ICML)}, 2016. \label{l1-icml16}

\item \textbf{Y. Zhang}, M. Wainwright and MI. Jordan. Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds.
\emph{International Conference on Machine Learning (ICML)}, 2015. \label{distributed-icml15}

\item \textbf{Y. Zhang} and L. Xiao. DiSCO: Communication-Efficient Distributed Optimization of Self-Concordant Loss.
\emph{International Conference on Machine Learning (ICML)}, 2015. \label{communication-icml15}

\item \textbf{Y. Zhang} and L. Xiao. Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization.
\emph{International Conference on Machine Learning (ICML)}, 2015.\label{stochastic-icml15}

\item \textbf{Y. Zhang}, X. Chen, D. Zhou and MI. Jordan. Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing.
\emph{Neural Information Processing Systems (NIPS)}, 2014. \textbf{(Spotlight presentation, 4.8\% acceptance rate)}
\label{spectral-nips14}

\item \textbf{Y. Zhang}, M. Wainwright and MI. Jordan. Lower Bounds on the Performance of Polynomial-time Algorithms for Sparse Linear Regression. \emph{Conference on Learning Theory (COLT)}, 2014. \label{lower-colt14}

\item \textbf{Y. Zhang}, A. Ahmed, V. Josifovski and A. Smola. Taxonomy Discovery for Personalized Recommendation. \emph{ACM International Conference on Web Search and Data Mining (WSDM)}, 2014. \label{taxonomy-wsdm14}

\item \textbf{Y. Zhang}, J. Duchi, M. Wainwright and MI. Jordan. Information-theoretic Lower Bounds for Distributed Statistical Estimation with Communication Constraints.
\emph{Neural Information Processing Systems (NIPS)}, 2013. \textbf{(Oral presentation, 1.4\% acceptance rate)}
\label{information-nips13}

\item  \textbf{Y. Zhang}, J. Duchi and M. Wainwright. Divide and Conquer Kernel Ridge Regression.
\emph{Conference on Learning Theory (COLT)}, 2013. \label{divide-colt13}

\item  \textbf{Y. Zhang}, J. Duchi and M. Wainwright. Communication-Efficient Algorithms for Statistical Optimization.
\emph{Neural Information Processing Systems (NIPS)}, 2012. \label{communication-nips12}

\item  W. Chen, D. Wang, \textbf{Y. Zhang} and Q. Yang. Understanding Click Noise: A Noise-aware Click Model for Web Search.
\emph{ACM International Conference on Web Search and Data Mining (WSDM)}, 2012. \label{understanding-wsdm12}

\item  \textbf{Y. Zhang}, W, Chen and D, Wang, Q. Yang. User-click Modeling for Understanding and Predicting Search-behavior.
\emph{ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)}, 2011.\label{user-kdd11}
\item B. Hu, \textbf{Y. Zhang}, G. Wang, Q. Yang, W. Chen. Characterize Search Intent Diversity into Click Models.
\emph{International World Wide Web Conference (WWW)}, 2011. \label{characterize-www11}

\item \textbf{Y. Zhang}, D. Wang, G. Wang, W. Chen, Z. Zhang, B. Hu and L. Zhang. Learning Click Model via Probit Bayesian Inference.
\emph{ACM International Conference on Information and Knowledge Management (CIKM)}, 2010. \label{learning-cikm10}

\item D. Wang, W. Chen, G. Wang, \textbf{Y Zhang} and B. Hu. Explore Click Models for Search Ranking.
\emph{ACM International Conference on Information and Knowledge Management (CIKM)}, short paper, 2010. \label{explore-cikm10}

\item F. Zhong, D. Wang, G. Wang, W. Chen, \textbf{Y. Zhang}, Z. Chen and H. Wang. Incorporating Post-Click Behaviors Into a Click Model.
\emph{Annual International ACM SIGIR Conference (SIGIR)}, 2010. \label{incorporating-sigir10}

\item \textbf{Y. Zhang} and L. Zhang. Extracting Independent Rules: a New Perspective of Boosting. 
\emph{International Symposium on Artificial Intelligence and Mathematics (ISAIM)}, 2010. \label{a-new-isaim10}
\end{enumerate}

{\bf\Large 技术报告}
\vspace{5pt}

\begin{enumerate}[label={[T\arabic*]}, ref={T\arabic*}]
\item \label{splash}\textbf{Y. Zhang} and MI. Jordan. Splash: User-friendly Programming Interface for Parallelizing Stochastic Algorithms. \emph{arXiv:1506.07552}, 2015.
\end{enumerate}

{\Large\bf 奖励和荣誉}

\vspace{-5pt}
\textbf{2017}~~~~~~~~最佳论文奖,  国际机器学习理论会议 (COLT)\\
\textbf{2016}~~~~~~~~杰出审稿人奖,  国际机器学习会议 (ICML)\\
\textbf{2015}~~~~~~~~百度奖学金 (世界范围遴选8名博士生)\\
\textbf{2006}~~~~~~~~亚洲物理奥林匹克：银牌\\
\textbf{2006}~~~~~~~~中国物理奥林匹克：金牌 (全国第5名)


{\Large\bf 学术服务}

\vspace{-5pt}
{\bf 学术期刊审稿人:} Journal of Machine Learning Research, Annals of Statistics, Mathematical Programming, ACM Transactions on the Web. \\
{\bf 学术会议审稿人:} ICML (2013 - ), NIPS (2013 - ), AISTAT (2015 - ), IJCAI (2015 - ).

\end{resume}
\end{document}

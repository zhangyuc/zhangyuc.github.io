\documentclass{res}
\setlength{\textheight}{8.6in} % increase text height to fit resume on 1 page
\newsectionwidth{0pt}  % So the text is not indented under section headings
\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,anchorcolor=blue,citecolor=blue]{hyperref}
\usepackage{enumitem}

\newenvironment{my_item}{
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}}
{\end{itemize}
}

\begin{document}

% Center the name over the entire width of resume:
 \moveleft.5\hoffset\centerline{\LARGE\bf Yuchen Zhang}
% Draw a horizontal line the whole width of resume:
 \moveleft\hoffset\vbox{\hrule width\resumewidth height 1pt}\smallskip
% address begins here
\textbf{Email}: zhangyuc@gmail.com \hfill \textbf{Phone}: (+1)-510-423-1353

\begin{resume}

{\Large\bf Education}

\vspace{-5pt}
\textbf{University of California, Berkeley}\hfill\textbf{2011 - 2016}\\
Ph.D. in Computer Science\\
{Advised by Michael I. Jordan and Martin J. Wainwright}\\
Research: distributed machine learning, optimization algorithms, deep learning theory

\vspace{-5pt}
\textbf{University of California, Berkeley} \hfill\textbf{2011 - 2013}\\
Master in Statistics

\vspace{-5pt}
\textbf{Tsinghua University}\hfill\textbf{2007 - 2011}\\
Bachelor in Computer Science\\
{Supervised by Andrew C. Yao (Yao Class)}

{\Large\bf Employment History}

\vspace{-5pt}
{\bf Senior Researcher at Microsoft}\hfill\textbf{2018 - Now}\\
\vspace{-10pt}
\begin{my_item}
\item My team builds a revolutionary dialogue system that understands complex long-tail queries, and seamlessly drives multi-turn conversations to communicate, collaborate, and accomplish tasks. Serving MS Office and 3rd-party parteners with AI assiatant and customer service bots (\href{https://v.youku.com/v_show/id_XNDE3MTExODQ0MA}{demo link}).
\item I lead the core representation and modeling efforts for our dialogue system. I am the main designer and developer of the core dialogue representation, and a major contributor to the system's capability of compositional semantic parsing, state tracking, error handling and natural language generation.

\end{my_item}
\vspace{-5pt}
{\bf Senior Research Scientist at Semantic Machines}\hfill\textbf{2018}\\
Semantic Machines is a team of world-class researchers and engineers working on the next-generation dialogue system technologies. We were acquired to continue our mission at Microsoft since June, 2018.\vspace{3pt}\\
{\bf Post-doc Researcher at Stanford University}\hfill\textbf{2016 - 2018}\\
{Research: question answering, semantic parsing, deep learning theory}\vspace{3pt}\\
{\bf Internships}\hfill\textbf{2010 - 2016}\\
Microsoft Research, Google, Baidu

{\bf\Large Research Projects}

\vspace{-10pt}
\paragraph{Natural language processing}
\begin{my_item}
\item Task-oriented dialogue systems [\ref{task-tacl}, \ref{updating-patent}, \ref{reponse-patent}].
\item Semantic parsing for question answering [\ref{macro-emnlp17}].
\end{my_item}

\vspace{-10pt}
\paragraph{Machine learning and optimization}
\begin{my_item}
\item Deep learning and non-convex optimization [\ref{convexified-icml17},\ref{a-hitting-colt17},\ref{on-the-learnability-aistats17},\ref{local-nips16},\ref{l1-icml16}].
\item Convex optimization [\ref{stochastic-jmlr},\ref{stochastic-icml15}].
\item Crowdsourcing [\ref{spectral-jmlr},\ref{spectral-nips14}].
\item Robust machine learning [\ref{defending-aistats17}].
\item Personalized recommender systems [\ref{taxonomy-wsdm14}].
\item Web search and online advertising [\ref{understanding-wsdm12},\ref{user-kdd11},\ref{characterize-www11},\ref{learning-cikm10},\ref{explore-cikm10},\ref{incorporating-sigir10}]
\end{my_item}

\vspace{-10pt}
\paragraph{Distributed computing}
\begin{my_item}
\item Distributed algorithms for machine learning [\ref{divide-jmlr},\ref{communication-jmlr},\ref{distributed-icml15},\ref{communication-icml15},\ref{divide-colt13},\ref{communication-nips12}].
\item Foundamental theory of distributed computing [\ref{lower-colt14},\ref{information-nips13}].
\item Programming interface for parallelizing stochastic algorithms [\ref{splash}].
\end{my_item}

\vspace{-10pt}
\paragraph{Other projects}
\begin{my_item}
\item Theoretical statistics [\ref{a-note-ejs},\ref{optimality-ejs},\ref{on-bayes-jmlr}].
\item Theoretical computer science [\ref{the-antimagicness-tcs},\ref{a-new-isaim10}].
\end{my_item}

{\bf\Large Patents}
\vspace{5pt}

\begin{enumerate}[label={[P\arabic*]}, ref={P\arabic*}]
\item \label{updating-patent} \textbf{Y. Zhang}, J. Wolfe, A. Pauls and D. Hall. Updating Constraints For Computerized Assistant Actions. \emph{U.S.~Patent}, Filed by Microsoft on March 30, 2020.
\item \label{reponse-patent} J. Andreas, A. Vorobev, A. Guo, J. Krishnamurthy, J. Bufe, J. Rusak and \textbf{Y. Zhang}. Response Generation For Conversational Computing Interface, \emph{U.S.~Patent}, Filed by Microsoft on August 12, 2019.
\end{enumerate}

{\bf\Large Journal Publications}
\vspace{5pt}

\begin{enumerate}[label={[J\arabic*]}, ref={J\arabic*}]

\item \textbf{Y. Zhang} and the Semantic Machines team (alpha-beta order). Task-Oriented Dialogue as Dataflow Synthesis.
\emph{Transactions of the Association for Computational Linguistics}.\label{task-tacl}

\item \textbf{Y. Zhang} and L. Xiao. Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization.
\emph{Journal of Machine Learning Research}.\label{stochastic-jmlr}

\item X. Chen, A. Guntuboyina and \textbf{Y. Zhang}. A note on the approximate admissibility of regularized estimators in the Gaussian sequence model.
\emph{Electronic Journal of Statistics}.\label{a-note-ejs}

\item \textbf{Y. Zhang}, M. Wainwright and MI. Jordan. Optimal prediction for sparse linear models? Lower bounds for coordinate-separable M-estimators.\label{optimality-ejs}
\emph{Electronic Journal of Statistics}.

\item X. Chen, A. Guntuboyina and \textbf{Y. Zhang} (alpha-beta order). On Bayes Risk Lower Bounds.
\emph{Journal of Machine Learning Research}.\label{on-bayes-jmlr}

\item \textbf{Y. Zhang}, X. Chen, D. Zhou and MI. Jordan. Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing.
\emph{Journal of Machine Learning Research}.\label{spectral-jmlr}

\item \textbf{Y. Zhang}, J. Duchi and M. Wainwright. Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates.
\emph{Journal of Machine Learning Research}.\label{divide-jmlr}

\item \textbf{Y. Zhang}, J. Duchi and M. Wainwright. Communication-Efficient Algorithms for Statistical Optimization.
\emph{Journal of Machine Learning Research}.\label{communication-jmlr}

\item \textbf{Y. Zhang} and X. Sun. The Antimagicness of the Cartesian Product of Graphs.
\emph{Theoretical Computer Science}.\label{the-antimagicness-tcs}
\end{enumerate}

{\bf\Large Conference Publications}
\vspace{5pt}

\begin{enumerate}[label={[C\arabic*]}, ref={C\arabic*}]

\item  \textbf{Y. Zhang}, P. Liang. Defending Against Whitebox Adversarial Attacks via Randomized Discretization.
\emph{Artificial Intelligence and Statistics (AISTATS)}, 2019. \label{defending-aistats17}

\item  \textbf{Y. Zhang}, P. Pasupat, P. Liang. Macro Grammars and Holistic Triggering for Efficient Semantic Parsing.
\emph{Empirical Methods on Natural Language Processing (EMNLP)}, 2017. \label{macro-emnlp17}

\item  \textbf{Y. Zhang}, P. Liang, M. Wainwright. Convexified Convolutional Neural Networks.
\emph{International Conference on Machine Learning (ICML)}, 2017. \label{convexified-icml17}

\item  \textbf{Y. Zhang}, P. Liang, M. Charikar. A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics.
\emph{Conference on Learning Theory (COLT)}, 2017 {\bf (Best paper award)}. \label{a-hitting-colt17}

\item \textbf{Y. Zhang}, JD. Lee, M. Wainwright and MI. Jordan. On the Learnability of Fully-connected Neural Networks.
\emph{Artificial Intelligence and Statistics (AISTATS)}, 2017. \label{on-the-learnability-aistats17}

\item C. Jin, {\bf Y. Zhang}, S. Balakrishnan, M. Wainwright, MI. Jordan.  
Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences.
\emph{Neural Information Processing Systems (NIPS)}, 2016. \label{local-nips16}

\item \textbf{Y. Zhang}, JD. Lee, MI. Jordan. $\ell_1$-regularized Neural Networks are Improperly Learnable in Polynomial Time.
\emph{International Conference on Machine Learning (ICML)}, 2016. \label{l1-icml16}

\item \textbf{Y. Zhang}, M. Wainwright and MI. Jordan. Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds.
\emph{International Conference on Machine Learning (ICML)}, 2015. \label{distributed-icml15}

\item \textbf{Y. Zhang} and L. Xiao. DiSCO: Communication-Efficient Distributed Optimization of Self-Concordant Loss.
\emph{International Conference on Machine Learning (ICML)}, 2015. \label{communication-icml15}

\item \textbf{Y. Zhang} and L. Xiao. Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization.
\emph{International Conference on Machine Learning (ICML)}, 2015.\label{stochastic-icml15}

\item \textbf{Y. Zhang}, X. Chen, D. Zhou and MI. Jordan. Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing.
\emph{Neural Information Processing Systems (NIPS)}, 2014. \textbf{(Spotlight presentation, 4.8\% acceptance rate)}
\label{spectral-nips14}

\item \textbf{Y. Zhang}, M. Wainwright and MI. Jordan. Lower Bounds on the Performance of Polynomial-time Algorithms for Sparse Linear Regression. \emph{Conference on Learning Theory (COLT)}, 2014. \label{lower-colt14}

\item \textbf{Y. Zhang}, A. Ahmed, V. Josifovski and A. Smola. Taxonomy Discovery for Personalized Recommendation. \emph{ACM International Conference on Web Search and Data Mining (WSDM)}, 2014. \label{taxonomy-wsdm14}

\item \textbf{Y. Zhang}, J. Duchi, M. Wainwright and MI. Jordan. Information-theoretic Lower Bounds for Distributed Statistical Estimation with Communication Constraints.
\emph{Neural Information Processing Systems (NIPS)}, 2013. \textbf{(Oral presentation, 1.4\% acceptance rate)}
\label{information-nips13}

\item  \textbf{Y. Zhang}, J. Duchi and M. Wainwright. Divide and Conquer Kernel Ridge Regression.
\emph{Conference on Learning Theory (COLT)}, 2013. \label{divide-colt13}

\item  \textbf{Y. Zhang}, J. Duchi and M. Wainwright. Communication-Efficient Algorithms for Statistical Optimization.
\emph{Neural Information Processing Systems (NIPS)}, 2012. \label{communication-nips12}

\item  W. Chen, D. Wang, \textbf{Y. Zhang} and Q. Yang. Understanding Click Noise: A Noise-aware Click Model for Web Search.
\emph{ACM International Conference on Web Search and Data Mining (WSDM)}, 2012. \label{understanding-wsdm12}

\item  \textbf{Y. Zhang}, W, Chen and D, Wang, Q. Yang. User-click Modeling for Understanding and Predicting Search-behavior.
\emph{ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)}, 2011.\label{user-kdd11}
\item B. Hu, \textbf{Y. Zhang}, G. Wang, Q. Yang, W. Chen. Characterize Search Intent Diversity into Click Models.
\emph{International World Wide Web Conference (WWW)}, 2011. \label{characterize-www11}

\item \textbf{Y. Zhang}, D. Wang, G. Wang, W. Chen, Z. Zhang, B. Hu and L. Zhang. Learning Click Model via Probit Bayesian Inference.
\emph{ACM International Conference on Information and Knowledge Management (CIKM)}, 2010. \label{learning-cikm10}

\item D. Wang, W. Chen, G. Wang, \textbf{Y Zhang} and B. Hu. Explore Click Models for Search Ranking.
\emph{ACM International Conference on Information and Knowledge Management (CIKM)}, short paper, 2010. \label{explore-cikm10}

\item F. Zhong, D. Wang, G. Wang, W. Chen, \textbf{Y. Zhang}, Z. Chen and H. Wang. Incorporating Post-Click Behaviors Into a Click Model.
\emph{Annual International ACM SIGIR Conference (SIGIR)}, 2010. \label{incorporating-sigir10}

\item \textbf{Y. Zhang} and L. Zhang. Extracting Independent Rules: a New Perspective of Boosting. 
\emph{International Symposium on Artificial Intelligence and Mathematics (ISAIM)}, 2010. \label{a-new-isaim10}
\end{enumerate}

{\bf\Large Technical Reports}
\vspace{5pt}

\begin{enumerate}[label={[T\arabic*]}, ref={T\arabic*}]
\item \label{splash}\textbf{Y. Zhang} and MI. Jordan. Splash: User-friendly Programming Interface for Parallelizing Stochastic Algorithms. \emph{arXiv:1506.07552}, 2015.
\end{enumerate}

{\Large\bf Selected Awards \& Honors}

\vspace{-5pt}
\textbf{2017}~~~~~~~~Best Paper Award, Conference on Learning Theory (COLT).\\
\textbf{2016}~~~~~~~~Outstanding Reviewer Award, International Conference on Machine Learning (ICML).\\
\textbf{2015}~~~~~~~~Baidu Fellowship (awards 8 PhD students every year worldwide).\\
\textbf{2013}~~~~~~~~Microsoft Research PhD Fellowship Finalist.\\
\textbf{2006}~~~~~~~~Silver Medal in Asian Physics Olympiad.\\
\textbf{2006}~~~~~~~~Gold Medal in Chinese Physics Olympiad ($5^{\rm{th}}$ among 400,000 participants).

{\Large\bf Teaching}

\vspace{-5pt}
Graduate Student Instructor, Introduction to machine learning, UC Berkeley \hfill\textbf{2015}\\
Graduate Student Instructor, Randomized algorithms for matrices and data, UC Berkeley \hfill\textbf{2013}

{\Large\bf Service}

\vspace{-5pt}
{\bf Journal Reviewer:} Journal of Machine Learning Research, Annals of Statistics, Mathematical Programming, ACM Transactions on the Web. \\
{\bf Conference Reviewer:} ICML (2013 - ), NIPS (2013 - ), AISTAT (2015 - ), IJCAI (2015 - ).

\end{resume}
\end{document}
